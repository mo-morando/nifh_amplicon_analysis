# nifh_amplicons_analysis

____

***nifh_amplicons_analysis*** contains the bioinformatic workflow of the analysis of the [*nifH* database](https://figshare.com/articles/dataset/Global_biogeography_of_N_sub_2_sub_-fixing_microbes_i_i_i_nifH_i_amplicon_database_and_analytics_workflow/23795943/1?file=46033371), most of which is described in [(Morando, Magasin) et al. 2025](https://essd.copernicus.org/preprints/essd-2024-163/). This workflow is managed by a Snakefile that creates a conda environment and then executes multiple R scripts that load and process the data before preforming the ecological and biogeochemical analysis of nifH ASVs and their metadata. Outputs are figures, tables, and data (csv) and log (txt) files that represent both work found in [(Morando, Magasin) et al. 2025](https://essd.copernicus.org/preprints/essd-2024-163/) as well as other information providing a broader view of the [*nifH* database](https://figshare.com/articles/dataset/Global_biogeography_of_N_sub_2_sub_-fixing_microbes_i_i_i_nifH_i_amplicon_database_and_analytics_workflow/23795943/1?file=46033371). This is still being updated and will expand over time.

## Data

____

The [*nifH* database](https://figshare.com/articles/dataset/Global_biogeography_of_N_sub_2_sub_-fixing_microbes_i_i_i_nifH_i_amplicon_database_and_analytics_workflow/23795943/1?file=46033371) consisted of nearly all published _nifH_ amplicon MiSeq data sets that existed at the time of publication, as well as two new data sets produced by the [Zehr Lab](https://www.jzehrlab.com/) at [UC Santa Cruz](https://www.ucsc.edu/). The samples are shown in the map below which links to an interactive Google map with more detailed information, e.g., study names, sample IDs, and collection information for each sample.
[![Map of studies used in Morando, Magasin et al. 2024](images_for_readme/Morando_Magasin_et_al_2024_studies_used.png)](https://www.google.com/maps/d/u/0/edit?mid=1OlWftvxU_o7Fy3nFsSJDcUlbEWSX_U0&usp=sharing)


## Workflows that generated data used in analysis

____

Two separate bioinformatic workflows were used to generate the data (the *nifH* database) used in this analysis, i.e., the ASV database and associated metadata.
- [*nifH-amplicons-DADA2*](https://github.com/jdmagasin/nifH_amplicons_DADA2): nifH DADA2 pipeline that aggregated and produced the intial ASV table
- [*nifH-ASV-workflow*](https://github.com/jdmagasin/nifH-ASV-workflow): the post processing pipeline that quality filtered and validated the data, acquired metadata via CMAP, and ultimately generated the *nifH* database

**An overview of these workflows**:
![Overview of DADA2 niH workflow](images_for_readme/workflow_overview.png)
[DADA2](https://benjjneb.github.io/dada2/) ASVs were created by our [DADA2 _nifH_ pipeline](https://github.com/jdmagasin/nifH_amplicons_DADA2) (green). Post-pipeline stages (lavender), each executed by a Makefile or Snakefile, were used to gather the ASVs from all studies, filter the ASVs for quality, and annotate them, as well as to download sample-colocated environmental data from the [Simons Collaborative Marine Atlas Project (CMAP)](https://simonscmap.com).  The _nifH_ ASV database generated by the workflow will support future research into N<sub>2</sub>-fixing marine microbes.  The published database and any updated versions are available within the WorkspaceStartup directory, both as nifH_ASV_database.tgz as well as the R image, workspace.RData.  The published database is also available at [https://doi.org/10.6084/m9.figshare.23795943.v1](https://doi.org/10.6084/m9.figshare.23795943.v1).  

## Installation

____

The *nifh_amplicons_analysis* workflow can be obtain by cloning repo.

```bash
git clone https://github.com/mo-morando/nifh_amplicons_analysis
cd nifh_amplicons_analysis
```

The analysis can then be carried out by executing a Snakefile located in the scripts directory. This requires the installation of Snakemake. We recommend using a package manager, e.g., conda/mamba, and creating a contained environment. This ensures that the analysis runs smoothly and avoids potential conflicts with other programs and packages. This analysis workflow has been tested with Snakemake version *8.27.1* and so we suggest using this, however, newer versions may work.

To create a conda environment name *snakemake*:

```bash
conda create --name snakemake snakemake=8.27.1
conda activate snakemake
```

However, if you are already using the [*nifH-ASV-workflow*](https://github.com/jdmagasin/nifH-ASV-workflow), it contains a conda environment with Snakemake already, so the above step is not necessary. You can simple use this existing environment.

To activate this conda environment:

```bash
conda activate nifH_ASV_workflow
```

## Running the analysis

____

Once this environment is created and activated, the entire analysis is managed by a Snakemake file that first creates its own conda environment (*nifh_amplicons_analysis*) and then executes 8 different R scripts to complete the analysis. This second conda environment is more complex, containing all the packages needed to run the analysis. The environment is only activated while the workflow is running and the Snakefile closes the environment once it is finished.

To enter into the appropriate directory, execute the Snakefile, and run the analysis:

```bash
cd scripts
snakemake -c1 --use-conda
```

The workflow will produce most of the figures and tables found in [(Morando, Magasin) et al. 2025](https://essd.copernicus.org/preprints/essd-2024-163/), as well as many other figures, tables, including useful information that is printed out in the log files. Each script generates its own log file named to correspond the specific module being run. These files contain more than just potential error messages, detailed information regarding the processing of the data and results of the analysis are also printed out. In particular, *basic_sample_stats.log* and *samp_n_tax_breakdown.log* have a lot of useful information regarding some stats on the samples in general, e.g., number of DNA or replicate samples, as well as breakdowns of their taxonomy supplied within its log file.

These output files can be found in their respective directories:

```bash

analysis/out_files/ # The majority of the tables (csv)
analysis/out_files/logs # All log files
analysis/out_files/plots # All plots
analysis/out_files/tables # Important tables specific to Morando, Magasin et al. 2025
```

### Troubleshooting and error handling

____

The workflow is self-contained and error tested so everything should run fine. If there is any issues or the user tries to run different data or change the analysis in someway, comprehensive documentation and robust error handling was implemented to facilitate debugging.
