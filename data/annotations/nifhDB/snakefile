## Copyright (C) 2023 Michael (Mo) Morando and Jonathan D. Magasin
##
## Annotate ASVs from stage FilterAuids using three tools from the "data/
## annotations/scripts" directory:
##  1. BlastnARB2017          - Blastn ASVs against the 2017 ARB nifH database
##  2. BlastxGenome879        - Blastx ASVs against the Zehr Lab's database with
##                              879 diazotroph genomes.
## #! FIXME: This is not currently working
## #! Instead, I am using an the original CART annotation prior to the
## #! generation of the new AUIDs and nothing is generated here. See more below
## #! in snakefile.
##  !3. NifHClustersFrank2016  - Determine nifH clusters for each ASV using the
##                              classification and regression tree (CART) approach
##                              of Frank et al. 2016.
##  4. Marine_NCD_cyano_refsequences
##                             -  223 nifH sequences from selected
##                                cyanobacteria and NCDs to provide increased
##                                resolution.
##  5. UCYNA_oligoreps         -  Forty-four UNCY-A oligotypes, thirty-eight
##                                from metagenome assembly, to provide increase ##                                resolution of UCYN-A ASVs.
##
##
##  Final product:             -  annotation_tab.csv
##
##
## This snakefile is run in two stages:
## Stage one - Generating BLAST databases
## Each of these databases are generated from fasta files specific to those mentioned above. These databases are then used to annotate the query the nifH database fasta to provide taxonomic identification from several sources. These individual annotation tables are then processed and merged into a unified annotiaton table. A consensus ID is then made for each AUID based on this unified annotation table.
##
## Usage:
##    general:
##      snakemake -c1 <rule>
##
##      snakemake -c1 all_make_databases
##      -generate BLAST databases
##
##      snakemake -c1 all
##      -generate unified nifh ASV database annotation table
##
##      Various intermediate steps
##
##      Cleaning rules:
##
##      superclean_make_databases
##
##      clean_blast
##
##      clean_filter_annotation_headers
##
##      clean_filter_annotation
##
##      clean_prefix_headers
##
##      clean_add_auid_key
##
##      clean_merge_annotations
##
##      superclean_merge_directory
##
##      clean_annotation_directories
##
##      clean_all
##
##      superclean_all
##


##    make                # Generate all annotation types.  Make auids.annot.tsv
##    make ARB2017        #   Do just the blastn against ARB 2017
##    make Genome879      #   Do just the blastx against Genome879
##    make Clusters       #   Do just the nifH cluster classifications
##    make NCD_cyano      #   Do just the blastn against 267 NCD and cyano ref nifH
##
##    make clean          # Remove auids.annot.tsv
##    make superclean     # Remove auids.annot.tsv and all annotation output dirs.
##


# logfiles will have timestamps
# Import the datetime module
import datetime


# Function to generate a timestamp
def get_timestamp():
    return datetime.datetime.now().strftime("%Y%m%d")


## Making blast databases


## ARB database generation
# Load parameters from JSON config
configfile: "config/config_blast_db_arb.json"
configfile: "config/config_blast_db_genome879.json"
configfile: "./config/config_blast_db_ncd_cyano.json"
configfile: "./config/config_blast_db_ucyna.json"


# directories
input_fasta_dir_arb = config[f"input_fasta_dir_arb"]
blast_database_dir_arb = expand(
    "{input_fasta_dir}{blast_database_dir}",
    input_fasta_dir=input_fasta_dir_arb,
    blast_database_dir=config[f"blast_database_dir_arb"],
)


rule make_database_arb:
    input:
        expand(
            "{input_fasta_dir}{input_fasta_db}",
            input_fasta_dir=input_fasta_dir_arb,
            input_fasta_db=config["input_fasta_db_arb"],
        ),
    output:
        blast_database_dir=directory(blast_database_dir_arb),
    params:
        blast_database=expand(
            "{blast_database_dir}{blast_database}",
            blast_database_dir=blast_database_dir_arb,
            blast_database=config["blast_database_arb"],
        ),
        database_type=config["database_type_arb"],
    log:
        expand(
            "{blast_database_dir}logs/{blast_database}_{timestamp}.log",
            timestamp=get_timestamp(),
            blast_database_dir=blast_database_dir_arb,
            blast_database=config["blast_database_arb"],
        ),
    shell:
        """
        python3 ../../annotations/scripts/make_blastdb_fun.py --input_fasta_file {input} --database_type {params.database_type} --blast_database {params.blast_database} >> {log} 2>&1
        """


## genome879 database generation
# Load parameters from JSON config
configfile: "config/config_blast_db_genome879.json"


# directories
input_fasta_dir_genome879 = config["input_fasta_dir_genome879"]
blast_database_dir_genome879 = expand(
    "{input_fasta_dir}{blast_database_dir}",
    input_fasta_dir=input_fasta_dir_genome879,
    blast_database_dir=config["blast_database_dir_genome879"],
)


rule make_database_genome879:
    input:
        expand(
            "{input_fasta_dir}{input_fasta_db}",
            input_fasta_dir=input_fasta_dir_genome879,
            input_fasta_db=config["input_fasta_db_genome879"],
        ),
    output:
        blast_database_dir=directory(blast_database_dir_genome879),
    params:
        blast_database=expand(
            "{blast_database_dir}{blast_database}",
            blast_database_dir=blast_database_dir_genome879,
            blast_database=config["blast_database_genome879"],
        ),
        database_type=config["database_type_genome879"],
    log:
        expand(
            "{blast_database_dir}logs/{blast_database}_{timestamp}.log",
            timestamp=get_timestamp(),
            blast_database_dir=blast_database_dir_genome879,
            blast_database=config["blast_database_genome879"],
        ),
    shell:
        """
        python3 ../../annotations/scripts/make_blastdb_fun.py --input_fasta_file {input} --database_type {params.database_type} --blast_database {params.blast_database} >> {log} 2>&1
        """


# Load parameters from JSON config
configfile: "config/config_blast_db_ncd_cyano.json"


input_fasta_dir_ncd_cyano = config["input_fasta_dir_ncd_cyano"]
blast_database_dir_ncd_cyano = expand(
    "{input_fasta_dir}{blast_database_dir}",
    input_fasta_dir=input_fasta_dir_ncd_cyano,
    blast_database_dir=config["blast_database_dir_ncd_cyano"],
)


rule make_database_ncd_cyano:
    input:
        expand(
            "{input_fasta_dir}{input_fasta_db}",
            input_fasta_dir=input_fasta_dir_ncd_cyano,
            input_fasta_db=config["input_fasta_db_ncd_cyano"],
        ),
    output:
        blast_database_dir=directory(blast_database_dir_ncd_cyano),
    params:
        blast_database=expand(
            "{blast_database_dir}{blast_database}",
            blast_database_dir=blast_database_dir_ncd_cyano,
            blast_database=config["blast_database_ncd_cyano"],
        ),
        database_type=config["database_type_ncd_cyano"],
    log:
        expand(
            "{blast_database_dir}logs/{blast_database}_{timestamp}.log",
            timestamp=get_timestamp(),
            blast_database_dir=blast_database_dir_ncd_cyano,
            blast_database=config["blast_database_ncd_cyano"],
        ),
    shell:
        """
        python3 ../../annotations/scripts/make_blastdb_fun.py --input_fasta_file {input} --database_type {params.database_type} --blast_database {params.blast_database} >> {log} 2>&1
        """


## UCYN-A oligos database generation


# Load parameters from JSON config
configfile: "config/config_blast_db_ucyna.json"


input_fasta_dir_ucyna = config["input_fasta_dir_ucyna"]
blast_database_dir_ucyna = expand(
    "{input_fasta_dir}{blast_database_dir}",
    input_fasta_dir=input_fasta_dir_ucyna,
    blast_database_dir=config["blast_database_dir_ucyna"],
)


rule make_database_ucyna:
    input:
        expand(
            "{input_fasta_dir}{input_fasta_db}",
            input_fasta_dir=input_fasta_dir_ucyna,
            input_fasta_db=config["input_fasta_db_ucyna"],
        ),
    output:
        # expand(
        #     "{blast_database_dir}{blast_database}.{ext}",
        #     blast_database_dir=blast_database_dir_ucyna,
        #     blast_database=config["blast_database_ucyna"],
        #     ext=["ndb", "nhr", "nin", "njs", "not", "nsq", "ntf", "nto"],
        # ),
        blast_database_dir=directory(blast_database_dir_ucyna),
    params:
        blast_database=expand(
            "{blast_database_dir}{blast_database}",
            blast_database_dir=blast_database_dir_ucyna,
            blast_database=config["blast_database_ucyna"],
        ),
        database_type=config["database_type_ucyna"],
        # blast_database=config["blast_database"],
    log:
        # "logs/filter_blast{timestamp}.log",
        expand(
            "{blast_database_dir}logs/{blast_database}_{timestamp}.log",
            timestamp=get_timestamp(),
            blast_database_dir=blast_database_dir_ucyna,
            blast_database=config["blast_database_ucyna"],
        ),
    shell:
        """
        python3 ../../annotations/scripts/make_blastdb_fun.py --input_fasta_file {input} --database_type {params.database_type} --blast_database {params.blast_database} >> {log} 2>&1
        """


# ## Rule to make all databases:
# ## This run seperately from the rest of the snakefile below
# ## Rule to make them all
# rule all_make_databases:
#     input:
#         blast_database_dir_arb,
#         blast_database_dir_genome879,
#         blast_database_dir_ncd_cyano,
#         blast_database_dir_ucyna,


# ## Clean rule for database generation
# rule clean_all_make_databases:
#     shell:
#         """
#         rm -r ../../databases/*/*_db
#         """


# _#############################################################################
# _#############################################################################
# _#############################################################################


# -# Now we need to blast our fasta file against these newly generated blast datbases.


# Load config files
#################################
configfile: "config/config_blast_ucyna.json"
configfile: "config/config_blast_arb.json"
configfile: "config/config_blast_genome879.json"
configfile: "config/config_cart.json"
configfile: "config/config_blast_ncd_cyano.json"


# List of databases to iterate over the blast and subsequent processing of these results prior to merging into one unified annotation table
blast_dbs = config["blast_processing_key"]

# Dictionary to store blast database directories which is used in other stages
full_blast_database_dir_ = {}

## Define the full path for the blast database with for loop:
for database in blast_dbs:
    # * define blast db directory withing loop
    full_blast_database_dir = expand(
        "{input_fasta_dir}{blast_database_dir}",
        input_fasta_dir=config[f"input_fasta_dir_{database}"],
        blast_database_dir=config[f"blast_database_dir_{database}"],
    )
    full_blast_database_dir_[database] = full_blast_database_dir


# -# Define a rule to collect all outputs
rule all_blast:
    input:
        expand(
            "{blast_output_dir}{output}",
            blast_output_dir=[config[f"blast_output_dir_{db}"] for db in blast_dbs],
            output=[config[f"final_blst_csv_{db}"] for db in blast_dbs],
        ),


# -# Dynamically create rules for each database
for database in blast_dbs:
    # configfile: f"config/config_blast_{database}.json"
    # * define blast output directory withing loop
    blast_output_dir = config[f"blast_output_dir_{database}"]

    rule:
        name:
            f"blast_{database}"
        input:
            fasta=config[f"input_fasta_blast_{database}"],
        output:
            expand(
                "{blast_output_dir}{output}",
                blast_output_dir=blast_output_dir,
                output=config[f"final_blst_csv_{database}"],
            ),
        params:
            blast_database_blast=expand(
                "{full_blast_database_dir}{blast_database}",
                full_blast_database_dir=full_blast_database_dir_[database],
                blast_database=config[f"blast_database_{database}"],
            ),
            output_csv=expand(
                "{full_blast_database_dir}{output_csv}",
                full_blast_database_dir=full_blast_database_dir_[database],
                output_csv=config[f"output_csv_{database}"],
            ),
            blast_type=config[f"blast_type_{database}"],
            num_threads=config[f"num_threads_{database}"],
        log:
            expand(
                "{blast_output_dir}logs/{blast_database}_blast_{timestamp}.log",
                timestamp=get_timestamp(),
                blast_database=config[f"blast_database_{database}"],
                blast_output_dir=blast_output_dir,
            ),
        shell:
            """
            bash ../scripts/blast_function.sh {input.fasta} {params.blast_database_blast} {params.blast_type} {params.num_threads} {params.output_csv} {output} > {log} 2>&1
            """


########################


# _### Filtering blast out to keep only columns we want ####


# Load config files
#################################
configfile: "./config/config_filt_blastcols_arb.json"


configfile: "./config/config_filt_blastcols_ucyna.json"


configfile: "./config/config_filt_blastcols_genome879.json"


configfile: "./config/config_filt_blastcols_ncd_cyano.json"


# Define rule to column-based filter them all
# Criteria passed via config file
# Will filter annotations headers to only keep columns of interest
rule all_filter_annotation_headers:
    input:
        expand(
            "{blast_output_dir}{output}",
            blast_output_dir=[config[f"blast_output_dir_{db}"] for db in blast_dbs],
            output=[config[f"cols_filt_blast_output_{db}"] for db in blast_dbs],
        ),


# Dynamically create rule for each database
for database in blast_dbs:
    # * define blast output directory withing loop
    blast_output_dir = config[f"blast_output_dir_{database}"]

    rule:
        name:
            f"filter_annotation_headers_{database}"
        input:
            expand(
                "{blast_output_dir}{output}",
                blast_output_dir=blast_output_dir,
                output=config[f"final_blst_csv_{database}"],
            ),
        params:
            headers_to_keep=config[f"headers_to_keep_{database}"],
        output:
            expand(
                "{blast_output_dir}{output}",
                blast_output_dir=blast_output_dir,
                output=config[f"cols_filt_blast_output_{database}"],
            ),
        log:
            expand(
                "{blast_output_dir}logs/filter_annotation_headers_{timestamp}.log",
                timestamp=get_timestamp(),
                blast_output_dir=blast_output_dir,
            ),
        shell:
            """
            python3 ../scripts/filter_csv_cols_by_header.py --input_csv {input} --filtered_csv {output} --headers_to_keep {params.headers_to_keep} >> {log} 2>&1
            """


# Load config files
#################################
configfile: "config/config_filt_blast_ucyna.json"


configfile: "config/config_filt_blast_arb.json"


configfile: "config/config_filt_blast_genome879.json"


configfile: "config/config_filt_blast_ncd_cyano.json"


# Define a rule to filter all annotations
# Criteria passed via config file
# Will filter annotations that do not reach a given threshold
rule all_filter_annotation:
    input:
        expand(
            "{blast_output_dir}{output}",
            blast_output_dir=[config[f"blast_output_dir_{db}"] for db in blast_dbs],
            output=[config[f"filtered_blast_output_{db}"] for db in blast_dbs],
        ),


# Dynamically create rules for each database
for database in blast_dbs:
    # * define blast output directory withing loop
    blast_output_dir = config[f"blast_output_dir_{database}"]

    rule:
        name:
            f"filter_annotation_{database}"
        input:
            expand(
                "{blast_output_dir}{output}",
                blast_output_dir=blast_output_dir,
                output=config[f"cols_filt_blast_output_{database}"],
            ),
        params:
            min_qcov=config[f"min_qcov_{database}"],
            min_pident=config[f"min_pident_{database}"],
        output:
            expand(
                "{blast_output_dir}{output}",
                blast_output_dir=blast_output_dir,
                output=config[f"filtered_blast_output_{database}"],
            ),
        log:
            expand(
                "{blast_output_dir}logs/filter_blast_{timestamp}.log",
                timestamp=get_timestamp(),
                blast_output_dir=blast_output_dir,
            ),
        shell:
            """
            python3 ../../annotations/scripts/filter_blast_output_fun.py --input_blast_csv {input} --filtered_blast_output {output} --min_qcov {params.min_qcov} --min_pident {params.min_pident} >> {log} 2>&1
            """


# Load config files
#################################
configfile: "./config/config_change_csv_header_ucyna.json"


configfile: "./config/config_change_csv_header_arb.json"


configfile: "./config/config_change_csv_header_genome879.json"


configfile: "./config/config_change_csv_header_ncd_cyano.json"


configfile: "./config/config_change_csv_header_cart.json"


# Define a rule to change all headers of each annotation file
# Prefix specific to each database is added to each header
rule all_prefix_headers:
    input:
        expand(
            "{blast_output_dir}{output}",
            blast_output_dir=[config[f"blast_output_dir_{db}"] for db in blast_dbs],
            output=[config[f"output_header_csv_{db}"] for db in blast_dbs],
        ),


# Dynamically create rules for each database
for database in blast_dbs:
    # * define blast output directory withing loop
    blast_output_dir = config[f"blast_output_dir_{database}"]
    # print(blast_output_dir)

    rule:
        name:
            f"database_prefix_{database}"
        input:
            expand(
                "{blast_output_dir}{output}",
                blast_output_dir=blast_output_dir,
                output=config[f"filtered_blast_output_{database}"],
            ),
        params:
            header_prefix=config[f"header_prefix_{database}"],
        output:
            expand(
                "{blast_output_dir}{output}",
                blast_output_dir=blast_output_dir,
                output=config[f"output_header_csv_{database}"],
            ),
        log:
            expand(
                "{blast_output_dir}logs/headers_{timestamp}.log",
                timestamp=get_timestamp(),
                blast_output_dir=blast_output_dir,
            ),
        shell:
            """
            bash ../../annotations/scripts/change_csv_headers.sh {input} {output} {params.header_prefix} > {log} 2>&1
            """


# ## Rule to add AUID key
# !# At some point, new AUIDs were generated but all my files and analysis need these old AUID IDs, so this key needs to be added


# Load config files
#################################
configfile: "./config/config_add_auid_key_ucyna.json"


configfile: "./config/config_add_auid_key_arb.json"


configfile: "./config/config_add_auid_key_genome879.json"


configfile: "./config/config_add_auid_key_ncd_cyano.json"


configfile: "./config/config_add_auid_key_cart.json"


# Define a rule to add AUID key to link old and new AUIDs between annotation files
rule all_add_auid_key:
    input:
        expand(
            "{blast_output_dir}{output}",
            blast_output_dir=[config[f"blast_output_dir_{db}"] for db in blast_dbs],
            output=[config[f"keyed_csv_{db}"] for db in blast_dbs],
        ),


# Dynamically create rules for each database
for database in blast_dbs:
    # * define blast output directory withing loop
    blast_output_dir = config[f"blast_output_dir_{database}"]

    rule:
        name:
            f"add_auid_key{database}"
        input:
            expand(
                "{blast_output_dir}{output}",
                blast_output_dir=blast_output_dir,
                output=config[f"output_header_csv_{database}"],
            ),
        params:
            left_on_key=config[f"left_on_key_{database}"],
            right_on_key=config[f"right_on_key_{database}"],
            how_key=config[f"how_key_{database}"],
        output:
            expand(
                "{blast_output_dir}{output}",
                blast_output_dir=blast_output_dir,
                output=config[f"keyed_csv_{database}"],
            ),
        log:
            # "logs/filter_blast{timestamp}.log",
            expand(
                "{blast_output_dir}logs/auid_key_{timestamp}.log",
                timestamp=get_timestamp(),
                blast_output_dir=blast_output_dir,
            ),
        shell:
            """
            python3 ../../annotations/scripts/add_auid_key.py --left_csv {input} --merged_csv {output} --left_on {params.left_on_key} --right_on {params.right_on_key} --how {params.how_key} >> {log} 2>&1
            """


########################

# _# Merging files
## A series of merging that concatenates all the annotation files into a
## single, unified annotation table
## Finally it adds some description files to add more information to the
## annotation file


# Load config files
#################################
configfile: "./config/config_merge_cart_arb.json"


configfile: "./config/config_merge_genome879.json"


configfile: "./config/config_merge_ncd_cyano.json"


configfile: "./config/config_merge_ucyna.json"


configfile: "./config/config_merge_descrp_ncd_cyano.json"


configfile: "./config/config_merge_descrp_genome879.json"


## define a merging key
merge_key = config["merge_key"]


##- rule to merge them all
rule all_merge_annotations:
    input:
        expand(
            "{merged_dir}{output}",
            merged_dir=config[f"merged_dir_{merge_key[6]}"],
            output=config[f"merged_csv_{merge_key[6]}"],
        ),


## Merge 1 cart with arb
merged_dir = config[f"merged_dir_{merge_key[0]}"]


rule:
    name:
        f"merge_annotations_1"
    input:
        right_csv=expand(
            "{blast_output_dir}{output}",
            blast_output_dir=config[f"blast_output_dir_{merge_key[0]}"],
            output=config[f"keyed_csv_{merge_key[0]}"],
        ),
        left_csv=config[f"left_csv_{merge_key[0]}"],
        # config["right_csv"],
    params:
        # left_csv=config[f"left_csv_{merge_key[0]}"],
        # left_on=config[f"left_on_{merge_key}"],
        left_on=config[f"left_on_{merge_key[0]}"],
        right_on=config[f"right_on_{merge_key[0]}"],
        how=config[f"how_{merge_key[0]}"],
    output:
        # config["merged_csv"],
        expand(
            "{merged_dir}{output}",
            merged_dir=merged_dir,
            output=config[f"merged_csv_{merge_key[0]}"],
        ),
    log:
        expand(
            "{merged_dir}logs/merge_{merge_key1}_{merge_key2}_{timestamp}.log",
            timestamp=get_timestamp(),
            merged_dir=merged_dir,
            merge_key1=f"{merge_key[0]}",
            merge_key2=f"{merge_key[1]}",
        ),
    shell:
        """
        python3 ../scripts/merge_csv.py --left_csv {input.left_csv} --right_csv {input.right_csv} --merged_csv {output} --left_on {params.left_on} --right_on {params.right_on} --how {params.how} >> {log} 2>&1
        """


### merge 2 merging output with genomes879

merged_dir = config[f"merged_dir_{merge_key[2]}"]
# blast_output_dir = config[f"blast_output_dir_{merge_key[0]}"]


rule:
    name:
        f"merge_annotations_2"
    input:
        # config["merged_csv"],
        right_csv=expand(
            "{merged_dir}{output}",
            merged_dir=config[f"merged_dir_{merge_key[0]}"],
            output=config[f"merged_csv_{merge_key[0]}"],
        ),
        left_csv=config[f"left_csv_{merge_key[2]}"],
    params:
        # left_csv=config[f"left_csv_{merge_key[2]}"],
        left_on=config[f"left_on_{merge_key[2]}"],
        right_on=config[f"right_on_{merge_key[2]}"],
        how=config[f"how_{merge_key[2]}"],
    output:
        # config["merged_csv"],
        expand(
            "{merged_dir}{output}",
            merged_dir=merged_dir,
            output=config[f"merged_csv_{merge_key[2]}"],
        ),
    log:
        expand(
            "{merged_dir}logs/merge_anno_{merge_key}_{timestamp}.log",
            timestamp=get_timestamp(),
            merged_dir=merged_dir,
            merge_key=f"{merge_key[2]}",
        ),
    shell:
        """
        python3 ../scripts/merge_csv.py --left_csv {input.left_csv} --right_csv {input.right_csv} --merged_csv {output} --left_on {params.left_on} --right_on {params.right_on} --how {params.how} >> {log} 2>&1
        """


### merge 3 merging output with ncd_cyanos

merged_dir = config[f"merged_dir_{merge_key[3]}"]
# blast_output_dir = config[f"blast_output_dir_{merge_key[0]}"]


rule:
    name:
        f"merge_annotations_3"
    input:
        # config["merged_csv"],
        right_csv=expand(
            "{merged_dir}{output}",
            merged_dir=config[f"merged_dir_{merge_key[0]}"],
            output=config[f"merged_csv_{merge_key[2]}"],
        ),
        left_csv=config[f"left_csv_{merge_key[3]}"],
    params:
        # left_csv=config[f"left_csv_{merge_key[3]}"],
        # left_on=config[f"left_on_{merge_key}"],
        left_on=config[f"left_on_{merge_key[3]}"],
        right_on=config[f"right_on_{merge_key[3]}"],
        how=config[f"how_{merge_key[3]}"],
    output:
        # config["merged_csv"],
        expand(
            "{merged_dir}{output}",
            merged_dir=merged_dir,
            output=config[f"merged_csv_{merge_key[3]}"],
        ),
    log:
        expand(
            "{merged_dir}logs/merge_anno_{merge_key}_{timestamp}.log",
            timestamp=get_timestamp(),
            merged_dir=merged_dir,
            merge_key=f"{merge_key[3]}",
        ),
    shell:
        """
        python3 ../scripts/merge_csv.py --left_csv {input.left_csv} --right_csv {input.right_csv} --merged_csv {output} --left_on {params.left_on} --right_on {params.right_on} --how {params.how} >> {log} 2>&1
        """


### merge 4 merge output with ucyna_oligos

merged_dir = config[f"merged_dir_{merge_key[4]}"]
# blast_output_dir = config[f"blast_output_dir_{merge_key[0]}"]


rule:
    name:
        f"merge_annotations_4"
    input:
        # config["merged_csv"],
        right_csv=expand(
            "{merged_dir}{output}",
            merged_dir=config[f"merged_dir_{merge_key[0]}"],
            output=config[f"merged_csv_{merge_key[3]}"],
        ),
        left_csv=config[f"left_csv_{merge_key[4]}"],
    params:
        # left_on=config[f"left_on_{merge_key}"],
        left_on=config[f"left_on_{merge_key[4]}"],
        right_on=config[f"right_on_{merge_key[4]}"],
        how=config[f"how_{merge_key[4]}"],
    output:
        # config["merged_csv"],
        expand(
            "{merged_dir}{output}",
            merged_dir=merged_dir,
            output=config[f"merged_csv_{merge_key[4]}"],
        ),
    log:
        expand(
            "{merged_dir}logs/merge_anno_{merge_key}_{timestamp}.log",
            timestamp=get_timestamp(),
            merged_dir=merged_dir,
            merge_key=f"{merge_key[4]}",
        ),
    shell:
        """
        python3 ../scripts/merge_csv.py --left_csv {input.left_csv} --right_csv {input.right_csv} --merged_csv {output} --left_on {params.left_on} --right_on {params.right_on} --how {params.how} >> {log} 2>&1
        """


####
# -# add description files
## merge ncd cyanos with its description file
# merged_dir = config[f"merged_dir_ncd_cyano"]
merged_dir = config[f"merged_dir_{merge_key[5]}"]


# blast_output_dir = config[f"blast_output_dir_{merge_key[0]}"]
rule:
    name:
        f"merge_annotations_5"
    input:
        right_csv=expand(
            "{merged_dir}{output}",
            merged_dir=merged_dir,
            output=config[f"merged_csv_{merge_key[4]}"],
        ),
        left_csv=expand(
            "{blast_database_dir}{input}",
            blast_database_dir=input_fasta_dir_ncd_cyano,
            input=config[f"left_csv_{merge_key[5]}"],
        ),
        # left_csv=config[f"left_csv_{merge_key[5]}"],
    params:
        # left_csv=config[f"left_csv_{merge_key[5]}"],
        # left_on=config[f"left_on_{merge_key}"],
        left_on=config[f"left_on_{merge_key[5]}"],
        right_on=config[f"right_on_{merge_key[5]}"],
        how=config[f"how_{merge_key[5]}"],
    output:
        # config["merged_csv"],
        expand(
            "{merged_dir}{output}",
            merged_dir=merged_dir,
            output=config[f"merged_csv_{merge_key[5]}"],
        ),
    log:
        expand(
            "{merged_dir}logs/merge_anno_{merge_key}_{timestamp}.log",
            timestamp=get_timestamp(),
            merged_dir=merged_dir,
            merge_key=f"{merge_key[5]}",
        ),
    shell:
        """
        python3 ../scripts/merge_csv.py --left_csv {input.left_csv} --right_csv {input.right_csv} --merged_csv {output} --left_on {params.left_on} --right_on {params.right_on} --how {params.how} >> {log} 2>&1
        """


## merge ncd cyanos with its description file
# merged_dir = config[f"merged_dir_ncd_cyano"]
merged_dir = config[f"merged_dir_{merge_key[6]}"]


# blast_output_dir = config[f"blast_output_dir_{merge_key[0]}"]
rule:
    name:
        f"merge_annotations_6"
    input:
        right_csv=expand(
            "{merged_dir}{output}",
            merged_dir=merged_dir,
            output=config[f"merged_csv_{merge_key[5]}"],
        ),
        left_csv=expand(
            "{blast_database_dir}{input}",
            blast_database_dir=input_fasta_dir_genome879,
            input=config[f"left_csv_{merge_key[6]}"],
        ),
        # left_csv=config[f"left_csv_{merge_key[6]}"],
    params:
        # left_csv=config[f"left_csv_{merge_key[6]}"],
        # left_on=config[f"left_on_{merge_key}"],
        left_on=config[f"left_on_{merge_key[6]}"],
        right_on=config[f"right_on_{merge_key[6]}"],
        how=config[f"how_{merge_key[6]}"],
    output:
        # config["merged_csv"],
        expand(
            "{merged_dir}{output}",
            merged_dir=merged_dir,
            output=config[f"merged_csv_{merge_key[6]}"],
        ),
    log:
        expand(
            "{merged_dir}logs/merge_anno_{merge_key}_{timestamp}.log",
            timestamp=get_timestamp(),
            merged_dir=merged_dir,
            merge_key=f"{merge_key[6]}",
        ),
    shell:
        """
        python3 ../scripts/merge_csv.py --left_csv {input.left_csv} --right_csv {input.right_csv} --merged_csv {output} --left_on {params.left_on} --right_on {params.right_on} --how {params.how} >> {log} 2>&1
        """


## Rule to make consensus ID for each ASV ##


# Load config files
#################################
configfile: "./config/config_consensus_id.json"


rule consensus_id:
    input:
        expand(
            "{merged_dir}{output}",
            merged_dir=merged_dir,
            output=config[f"merged_csv_{merge_key[6]}"],
        ),
    params:
        min_pid_genomes879=config["min_pid_genomes879"],
    output:
        # config["merged_csv"],
        expand(
            "{merged_dir}{output}",
            merged_dir=merged_dir,
            output=config["updated_annotation_tab"],
        ),
    log:
        expand(
            "{merged_dir}logs/consensus_id_{timestamp}.log",
            timestamp=get_timestamp(),
            merged_dir=merged_dir,
        ),
    shell:
        """
        python3 ../../annotations/scripts/make_consensus_taxonomy.py --annotation_table {input} --output_table {output}  --min_pid_genomes879 {params.min_pid_genomes879} >> {log} 2>&1
        """


rule all_make_databases:
    input:
        blast_database_dir_arb,
        blast_database_dir_genome879,
        blast_database_dir_ncd_cyano,
        blast_database_dir_ucyna,


rule all:
    input:
        # blast_database_dir_arb,
        # blast_database_dir_genome879,
        # blast_database_dir_ncd_cyano,
        # blast_database_dir_ucyna,
        expand(
            "{merged_dir}{output}",
            merged_dir=merged_dir,
            output=config["updated_annotation_tab"],
        ),


# _# Cleaning rules
# _# Cleaning rules
# _# Cleaning rules


## Clean rule for database generation
rule superclean_make_databases:
    shell:
        """
        echo "SUPER cleaning all_make_databases..."
        echo "Deleting all 'all_make_databases' files and directories..."
        snakemake -c1 --delete-all-output all_make_databases
        """


# ## SUPER Clean rule for database generation
# rule superclean_make_databases:
#     shell:
#         """
#         echo "SUPER cleaning all_make_databases..."
#         echo "Deleting all files and directories..."
#         rm -r ../../databases/*/*_db
#         """


rule clean_blast:
    shell:
        """
        echo "Cleaning all_blast outputfiles..."
        snakemake -c1 --delete-all-output all_blast
        """


rule clean_filter_annotation_headers:
    shell:
        """
        echo "Cleaning all_filter_annotation_headers outputfiles..."
        echo "Deleting all output files..."
        snakemake -c1 --delete-all-output all_filter_annotation_headers
        """


rule clean_filter_annotation:
    shell:
        """
        echo "Cleaning all_filter_annotation output files..."
        echo "Deleting all output files..."
        snakemake -c1 --delete-all-output all_filter_annotation
        """


rule clean_prefix_headers:
    shell:
        """
        echo "Cleaning all_prefix_headers output files..."
        echo "Deleting all output files..."
        snakemake -c1 --delete-all-output all_prefix_headers
        """


rule clean_add_auid_key:
    shell:
        """
        echo "Cleaning all_add_auid_key output files..."
        echo "Deleting all output files..."
        snakemake -c1 --delete-all-output all_add_auid_key
        """


rule clean_merge_annotations:
    shell:
        """
        echo "Cleaning all_merge_annotations output files..."
        echo "Deleting all output files..."
        snakemake -c1 --delete-all-output all_merge_annotations
        """


rule superclean_merge_directory:
    shell:
        """
        echo "SUPER cleaning 'merge' directory..."
        echo "Deleting entire 'merge' directory..."
        rm -r merged_database
        """


rule clean_annotation_directories:
    shell:
        """
        rm -r arb_nifh_2017 genome879 ncd_cyano UCYNA_oligos
        """


rule clean_all:
    shell:
        """
        echo "Cleaning 'all' output files..."
        echo "Deleting 'all' output files..."
        snakemake -c1 --delete-all-output all
        """


rule superclean_all:
    shell:
        """
        echo "SUPER cleaning all..."
        echo "Deleting all files and directories..."
        snakemake -c1 --delete-all-output all
        rm -r merged_database
        rm -r arb_nifh_2017 genome879 ncd_cyano UCYNA_oligos
        """
